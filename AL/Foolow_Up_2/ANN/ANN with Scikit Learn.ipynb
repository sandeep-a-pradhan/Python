{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    " \n",
    "Neural Networks are a machine learning framework that attempts to mimic the learning pattern of natural biological neural networks. Biological neural networks have interconnected neurons with dendrites that receive inputs, then based on these inputs they produce an output signal through an axon to another neuron. We will try to mimic this process through the use of Artificial Neural Networks (ANN), which we will just refer to as neural networks from now on. The process of creating a neural network begins with the most basic form, a single perceptron.\n",
    "\n",
    "The Perceptron\n",
    " \n",
    "Let's start our discussion by talking about the Perceptron! A perceptron has one or more inputs, a bias, an activation function, and a single output. The perceptron receives inputs, multiplies them by some weight, and then passes them into an activation function to produce an output. There are many possible activation functions to choose from, such as the logistic function, a trigonometric function, a step function etc. We also make sure to add a bias to the perceptron, this avoids issues where all inputs could be equal to zero (meaning no multiplicative weight would have an effect). Check out the diagram below for a visualization of a perceptron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the output we can compare it to a known label and adjust the weights accordingly (the weights usually start off with random initialization values). We keep repeating this process until we have reached a maximum number of allowed iterations, or an acceptable error rate.\n",
    "\n",
    "To create a neural network, we simply begin to add layers of perceptrons together, creating a multi-layer perceptron model of a neural network. You'll have an input layer which directly takes in your feature inputs and an output layer which will create the resulting outputs. Any layers in between are known as hidden layers because they don't directly \"see\" the feature inputs or outputs. For a visualization of this check out the diagram below (source: Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to actually creating a neural network with Python!\n",
    "\n",
    "SciKit-Learn\n",
    " \n",
    "In order to follow along with this tutorial, you'll need to have the latest version of SciKit Learn installed! It is easily installable either through pip or conda, but you can reference the official installation documentation for complete details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    " \n",
    "We'll use SciKit Learn's built in Breast Cancer Data Set which has several features of tumors with a labeled class indicating whether the tumor was Malignant or Benign. We will try to create a neural network model that can take in these features and attempt to predict malignant or benign labels for tumors it has not seen before. Let's go ahead and start by getting the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is like a dictionary, it contains a description of the data and the features and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target_names', 'data', 'target', 'DESCR', 'feature_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_names': array(['malignant', 'benign'],\n",
      "      dtype='|S9'), 'data': array([[  1.79900000e+01,   1.03800000e+01,   1.22800000e+02, ...,\n",
      "          2.65400000e-01,   4.60100000e-01,   1.18900000e-01],\n",
      "       [  2.05700000e+01,   1.77700000e+01,   1.32900000e+02, ...,\n",
      "          1.86000000e-01,   2.75000000e-01,   8.90200000e-02],\n",
      "       [  1.96900000e+01,   2.12500000e+01,   1.30000000e+02, ...,\n",
      "          2.43000000e-01,   3.61300000e-01,   8.75800000e-02],\n",
      "       ..., \n",
      "       [  1.66000000e+01,   2.80800000e+01,   1.08300000e+02, ...,\n",
      "          1.41800000e-01,   2.21800000e-01,   7.82000000e-02],\n",
      "       [  2.06000000e+01,   2.93300000e+01,   1.40100000e+02, ...,\n",
      "          2.65000000e-01,   4.08700000e-01,   1.24000000e-01],\n",
      "       [  7.76000000e+00,   2.45400000e+01,   4.79200000e+01, ...,\n",
      "          0.00000000e+00,   2.87100000e-01,   7.03900000e-02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "       0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "       1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "       1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]), 'DESCR': 'Breast Cancer Wisconsin (Diagnostic) Database\\n=============================================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry \\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\\n        13 is Radius SE, field 23 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\nReferences\\n----------\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.\\n', 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
      "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
      "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
      "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
      "       'smoothness error', 'compactness error', 'concavity error',\n",
      "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
      "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
      "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
      "       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n",
      "      dtype='|S23')}\n"
     ]
    }
   ],
   "source": [
    "print cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569L, 30L)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print full description by running:\n",
    "# print(cancer['DESCR'])\n",
    "# 569 data points with 30 features\n",
    "cancer['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Wisconsin (Diagnostic) Database\n",
      "=============================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      "References\n",
      "----------\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cancer['data']\n",
    "y = cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.79900000e+01,   1.03800000e+01,   1.22800000e+02, ...,\n",
       "          2.65400000e-01,   4.60100000e-01,   1.18900000e-01],\n",
       "       [  2.05700000e+01,   1.77700000e+01,   1.32900000e+02, ...,\n",
       "          1.86000000e-01,   2.75000000e-01,   8.90200000e-02],\n",
       "       [  1.96900000e+01,   2.12500000e+01,   1.30000000e+02, ...,\n",
       "          2.43000000e-01,   3.61300000e-01,   8.75800000e-02],\n",
       "       ..., \n",
       "       [  1.66000000e+01,   2.80800000e+01,   1.08300000e+02, ...,\n",
       "          1.41800000e-01,   2.21800000e-01,   7.82000000e-02],\n",
       "       [  2.06000000e+01,   2.93300000e+01,   1.40100000e+02, ...,\n",
       "          2.65000000e-01,   4.08700000e-01,   1.24000000e-01],\n",
       "       [  7.76000000e+00,   2.45400000e+01,   4.79200000e+01, ...,\n",
       "          0.00000000e+00,   2.87100000e-01,   7.03900000e-02]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split\n",
    " \n",
    "Let's split our data into training and testing sets, this is done easily with SciKit Learn's train_test_split function from model_selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    " \n",
    "The neural network may have difficulty converging before the maximum number of iterations allowed if the data is not normalized. Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. Note that you must apply the same scaling to the test set for meaningful results. There are a lot of different methods for normalization of data, we will use the built-in StandardScaler for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train our model. SciKit Learn makes this incredibly easy, by using estimator objects. In this case we will import our estimator (the Multi-Layer Perceptron Classifier model) from the neural_network library of SciKit-Learn!\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an instance of the model, there are a lot of parameters you can choose to define and customize here, we will only define the hidden_layer_sizes. For this parameter you pass in a tuple consisting of the number of neurons you want at each layer, where the nth entry in the tuple represents the number of neurons in the nth layer of the MLP model. There are many ways to choose these numbers, but for simplicity we will choose 3 layers with the same number of neurons as there are features in our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLPClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been made we can fit the training data to our model, remember that this data has already been processed and scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the output that shows the default values of the other parameters in the model. I encourage you to play around with them and discover what effects they have on your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Evaluation\n",
    "Now that we have a model it is time to use it to get predictions! We can do this simply with the predict() method off of our fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use SciKit-Learn's built in metrics such as a classification report and confusion matrix to evaluate how well our model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56  2]\n",
      " [ 3 82]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.97      0.96        58\n",
      "          1       0.98      0.96      0.97        85\n",
      "\n",
      "avg / total       0.97      0.97      0.97       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we only misclassified 3 tumors, leaving us with a 98% accuracy rate (as well as 98% precision and recall). This is pretty good considering how few lines of code we had to write! The downside however to using a Multi-Layer Preceptron model is how difficult it is to interpret the model itself. The weights and biases won't be easily interpretable in relation to which features are important to the model itself.\n",
    "\n",
    "However, if you do want to extract the MLP weights and biases after training your model, you use its public attributes coefs_ and intercepts_.\n",
    "\n",
    "coefs_ is a list of weight matrices, where weight matrix at index i represents the weights between layer i and layer i+1.\n",
    "\n",
    "intercepts_ is a list of bias vectors, where the vector at index i represents the bias values added to layer i+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.coefs_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.intercepts_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HR Case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "hr_df = pd.read_csv( 'HR_comma_sep.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding Categorical Features\n",
    "numerical_features = ['satisfaction_level', 'last_evaluation', 'number_project',\n",
    "     'average_montly_hours', 'time_spend_company']\n",
    "\n",
    "categorical_features = ['Work_accident','promotion_last_5years', 'department', 'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An utility function to create dummy variable\n",
    "def create_dummies( df, colname ):\n",
    "    col_dummies = pd.get_dummies(df[colname], prefix=colname)\n",
    "    col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n",
    "    df = pd.concat([df, col_dummies], axis=1)\n",
    "    df.drop( colname, axis = 1, inplace = True )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c_feature in categorical_features:\n",
    "  hr_df = create_dummies( hr_df, c_feature )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "\n",
    "feature_columns = hr_df.columns.difference( ['left'] )\n",
    "feature_columns1 = feature_columns[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split( hr_df[feature_columns],\n",
    "                                                  hr_df['left'],\n",
    "                                                  test_size = 0.2,\n",
    "                                                  random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a confusion matrix\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(train_X)\n",
    "X_test = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55296445\n",
      "Iteration 2, loss = 0.52898148\n",
      "Iteration 3, loss = 0.51605617\n",
      "Iteration 4, loss = 0.50016269\n",
      "Iteration 5, loss = 0.47897298\n",
      "Iteration 6, loss = 0.45734926\n",
      "Iteration 7, loss = 0.43893145\n",
      "Iteration 8, loss = 0.42313194\n",
      "Iteration 9, loss = 0.40929472\n",
      "Iteration 10, loss = 0.39651844\n",
      "Iteration 11, loss = 0.38486090\n",
      "Iteration 12, loss = 0.37419872\n",
      "Iteration 13, loss = 0.36434304\n",
      "Iteration 14, loss = 0.35517988\n",
      "Iteration 15, loss = 0.34680184\n",
      "Iteration 16, loss = 0.33896633\n",
      "Iteration 17, loss = 0.33069394\n",
      "Iteration 18, loss = 0.32225558\n",
      "Iteration 19, loss = 0.31170870\n",
      "Iteration 20, loss = 0.29946781\n",
      "Iteration 21, loss = 0.28604618\n",
      "Iteration 22, loss = 0.27147215\n",
      "Iteration 23, loss = 0.25682261\n",
      "Iteration 24, loss = 0.24468986\n",
      "Iteration 25, loss = 0.23505262\n",
      "Iteration 26, loss = 0.22737318\n",
      "Iteration 27, loss = 0.22100051\n",
      "Iteration 28, loss = 0.21552018\n",
      "Iteration 29, loss = 0.21080868\n",
      "Iteration 30, loss = 0.20667423\n",
      "Iteration 31, loss = 0.20293949\n",
      "Iteration 32, loss = 0.19960825\n",
      "Iteration 33, loss = 0.19652464\n",
      "Iteration 34, loss = 0.19381557\n",
      "Iteration 35, loss = 0.19139284\n",
      "Iteration 36, loss = 0.18915159\n",
      "Iteration 37, loss = 0.18709216\n",
      "Iteration 38, loss = 0.18522344\n",
      "Iteration 39, loss = 0.18356730\n",
      "Iteration 40, loss = 0.18187389\n",
      "Iteration 41, loss = 0.18050489\n",
      "Iteration 42, loss = 0.17909921\n",
      "Iteration 43, loss = 0.17791147\n",
      "Iteration 44, loss = 0.17681742\n",
      "Iteration 45, loss = 0.17576315\n",
      "Iteration 46, loss = 0.17473996\n",
      "Iteration 47, loss = 0.17391325\n",
      "Iteration 48, loss = 0.17303124\n",
      "Iteration 49, loss = 0.17229008\n",
      "Iteration 50, loss = 0.17160310\n",
      "Iteration 51, loss = 0.17091510\n",
      "Iteration 52, loss = 0.17031084\n",
      "Iteration 53, loss = 0.16973947\n",
      "Iteration 54, loss = 0.16909402\n",
      "Iteration 55, loss = 0.16864667\n",
      "Iteration 56, loss = 0.16815321\n",
      "Iteration 57, loss = 0.16772571\n",
      "Iteration 58, loss = 0.16724218\n",
      "Iteration 59, loss = 0.16680089\n",
      "Iteration 60, loss = 0.16637590\n",
      "Iteration 61, loss = 0.16602216\n",
      "Iteration 62, loss = 0.16571526\n",
      "Iteration 63, loss = 0.16539212\n",
      "Iteration 64, loss = 0.16498191\n",
      "Iteration 65, loss = 0.16467409\n",
      "Iteration 66, loss = 0.16434711\n",
      "Iteration 67, loss = 0.16408967\n",
      "Iteration 68, loss = 0.16377217\n",
      "Iteration 69, loss = 0.16347651\n",
      "Iteration 70, loss = 0.16327360\n",
      "Iteration 71, loss = 0.16292199\n",
      "Iteration 72, loss = 0.16274338\n",
      "Iteration 73, loss = 0.16257718\n",
      "Iteration 74, loss = 0.16224283\n",
      "Iteration 75, loss = 0.16207639\n",
      "Iteration 76, loss = 0.16181877\n",
      "Iteration 77, loss = 0.16159002\n",
      "Iteration 78, loss = 0.16139142\n",
      "Iteration 79, loss = 0.16105994\n",
      "Iteration 80, loss = 0.16089619\n",
      "Iteration 81, loss = 0.16071792\n",
      "Iteration 82, loss = 0.16050997\n",
      "Iteration 83, loss = 0.16042135\n",
      "Iteration 84, loss = 0.16007848\n",
      "Iteration 85, loss = 0.15993794\n",
      "Iteration 86, loss = 0.15973613\n",
      "Iteration 87, loss = 0.15953724\n",
      "Iteration 88, loss = 0.15931581\n",
      "Iteration 89, loss = 0.15908094\n",
      "Iteration 90, loss = 0.15886847\n",
      "Iteration 91, loss = 0.15871668\n",
      "Iteration 92, loss = 0.15849443\n",
      "Iteration 93, loss = 0.15840237\n",
      "Iteration 94, loss = 0.15822355\n",
      "Iteration 95, loss = 0.15797249\n",
      "Iteration 96, loss = 0.15786024\n",
      "Iteration 97, loss = 0.15756110\n",
      "Iteration 98, loss = 0.15733715\n",
      "Iteration 99, loss = 0.15718886\n",
      "Iteration 100, loss = 0.15701544\n",
      "Iteration 101, loss = 0.15683100\n",
      "Iteration 102, loss = 0.15665143\n",
      "Iteration 103, loss = 0.15642731\n",
      "Iteration 104, loss = 0.15628644\n",
      "Iteration 105, loss = 0.15608145\n",
      "Iteration 106, loss = 0.15586569\n",
      "Iteration 107, loss = 0.15567252\n",
      "Iteration 108, loss = 0.15552110\n",
      "Iteration 109, loss = 0.15528915\n",
      "Iteration 110, loss = 0.15518358\n",
      "Iteration 111, loss = 0.15499736\n",
      "Iteration 112, loss = 0.15489612\n",
      "Iteration 113, loss = 0.15475202\n",
      "Iteration 114, loss = 0.15457736\n",
      "Iteration 115, loss = 0.15437894\n",
      "Iteration 116, loss = 0.15420877\n",
      "Iteration 117, loss = 0.15401196\n",
      "Iteration 118, loss = 0.15389116\n",
      "Iteration 119, loss = 0.15374932\n",
      "Iteration 120, loss = 0.15359332\n",
      "Iteration 121, loss = 0.15341396\n",
      "Iteration 122, loss = 0.15327323\n",
      "Iteration 123, loss = 0.15313790\n",
      "Iteration 124, loss = 0.15292452\n",
      "Iteration 125, loss = 0.15284055\n",
      "Iteration 126, loss = 0.15271095\n",
      "Iteration 127, loss = 0.15255128\n",
      "Iteration 128, loss = 0.15241363\n",
      "Iteration 129, loss = 0.15223681\n",
      "Iteration 130, loss = 0.15209806\n",
      "Iteration 131, loss = 0.15206609\n",
      "Iteration 132, loss = 0.15182643\n",
      "Iteration 133, loss = 0.15169798\n",
      "Iteration 134, loss = 0.15154290\n",
      "Iteration 135, loss = 0.15142213\n",
      "Iteration 136, loss = 0.15124504\n",
      "Iteration 137, loss = 0.15098552\n",
      "Iteration 138, loss = 0.15091867\n",
      "Iteration 139, loss = 0.15078176\n",
      "Iteration 140, loss = 0.15054234\n",
      "Iteration 141, loss = 0.15046848\n",
      "Iteration 142, loss = 0.15038200\n",
      "Iteration 143, loss = 0.15017727\n",
      "Iteration 144, loss = 0.15001548\n",
      "Iteration 145, loss = 0.14993315\n",
      "Iteration 146, loss = 0.14990743\n",
      "Iteration 147, loss = 0.14974494\n",
      "Iteration 148, loss = 0.14968495\n",
      "Iteration 149, loss = 0.14945126\n",
      "Iteration 150, loss = 0.14934316\n",
      "Iteration 151, loss = 0.14923603\n",
      "Iteration 152, loss = 0.14911177\n",
      "Iteration 153, loss = 0.14905508\n",
      "Iteration 154, loss = 0.14883382\n",
      "Iteration 155, loss = 0.14878520\n",
      "Iteration 156, loss = 0.14863966\n",
      "Iteration 157, loss = 0.14862159\n",
      "Iteration 158, loss = 0.14844833\n",
      "Iteration 159, loss = 0.14835142\n",
      "Iteration 160, loss = 0.14826399\n",
      "Iteration 161, loss = 0.14815004\n",
      "Iteration 162, loss = 0.14804765\n",
      "Iteration 163, loss = 0.14797833\n",
      "Iteration 164, loss = 0.14787315\n",
      "Iteration 165, loss = 0.14793155\n",
      "Iteration 166, loss = 0.14762905\n",
      "Iteration 167, loss = 0.14759948\n",
      "Iteration 168, loss = 0.14765644\n",
      "Iteration 169, loss = 0.14740774\n",
      "Iteration 170, loss = 0.14726904\n",
      "Iteration 171, loss = 0.14728151\n",
      "Iteration 172, loss = 0.14712884\n",
      "Iteration 173, loss = 0.14706781\n",
      "Iteration 174, loss = 0.14699942\n",
      "Iteration 175, loss = 0.14690912\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=3, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(3), verbose=True)\n",
    "mlp.fit(X_train,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_backprop',\n",
       " '_compute_loss_grad',\n",
       " '_estimator_type',\n",
       " '_fit',\n",
       " '_fit_lbfgs',\n",
       " '_fit_stochastic',\n",
       " '_forward_pass',\n",
       " '_get_param_names',\n",
       " '_init_coef',\n",
       " '_initialize',\n",
       " '_label_binarizer',\n",
       " '_loss_grad_lbfgs',\n",
       " '_no_improvement_count',\n",
       " '_optimizer',\n",
       " '_partial_fit',\n",
       " '_predict',\n",
       " '_random_state',\n",
       " '_unpack',\n",
       " '_update_no_improvement_count',\n",
       " '_validate_hyperparameters',\n",
       " '_validate_input',\n",
       " 'activation',\n",
       " 'alpha',\n",
       " 'batch_size',\n",
       " 'best_loss_',\n",
       " 'beta_1',\n",
       " 'beta_2',\n",
       " 'classes_',\n",
       " 'coefs_',\n",
       " 'early_stopping',\n",
       " 'epsilon',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'hidden_layer_sizes',\n",
       " 'intercepts_',\n",
       " 'learning_rate',\n",
       " 'learning_rate_init',\n",
       " 'loss',\n",
       " 'loss_',\n",
       " 'loss_curve_',\n",
       " 'max_iter',\n",
       " 'momentum',\n",
       " 'n_iter_',\n",
       " 'n_layers_',\n",
       " 'n_outputs_',\n",
       " 'nesterovs_momentum',\n",
       " 'out_activation_',\n",
       " 'partial_fit',\n",
       " 'power_t',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'random_state',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'shuffle',\n",
       " 'solver',\n",
       " 't_',\n",
       " 'tol',\n",
       " 'validation_fraction',\n",
       " 'verbose',\n",
       " 'warm_start']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.39740484,  0.71596906,  0.93882086]), array([-3.14096063])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2232   62]\n",
      " [  81  625]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.97      2294\n",
      "          1       0.91      0.89      0.90       706\n",
      "\n",
      "avg / total       0.95      0.95      0.95      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(test_y,predictions))\n",
    "print(classification_report(test_y,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp.coefs_)\n",
    "len(mlp.coefs_[0])\n",
    "len(mlp.intercepts_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ -2.69084873e-01,  -2.81888328e-01,  -1.21388090e-02],\n",
       "        [  3.84538476e-01,   1.86983408e+00,   6.98924552e-01],\n",
       "        [ -6.68781010e-02,  -8.65197994e-02,  -1.66250605e-03],\n",
       "        [  2.49899747e-02,  -7.41896575e-03,  -6.06193254e-03],\n",
       "        [  4.73618326e-02,   1.25515915e-01,   6.73489237e-03],\n",
       "        [  1.20540013e-02,  -5.38384787e-02,  -8.75811887e-03],\n",
       "        [  9.48848286e-02,   5.58788753e-02,   2.42207396e-02],\n",
       "        [  3.82144570e-02,  -6.54589857e-02,   2.25251406e-02],\n",
       "        [  1.05218603e-01,   5.35527382e-02,   2.72690655e-03],\n",
       "        [  2.98835183e-02,   2.12068751e-01,   3.41058724e-02],\n",
       "        [  1.21668693e-01,   1.05674057e-01,   1.86818597e-02],\n",
       "        [ -1.61234943e-01,   1.40241940e+00,   4.39421810e-01],\n",
       "        [ -1.91701291e+00,   1.01465371e+00,  -7.37519707e-02],\n",
       "        [ -1.06013676e-01,  -3.60713265e-01,  -1.06127569e-01],\n",
       "        [  3.26743452e-01,   6.53906271e-01,  -7.17695668e-02],\n",
       "        [  2.53764521e-01,   5.82557312e-01,  -5.27496544e-02],\n",
       "        [ -3.30874935e-01,   8.33485353e-01,   6.30924816e-01],\n",
       "        [ -2.63688654e+00,  -1.49043423e+00,  -2.30101251e+00]]),\n",
       " array([[ 1.6345624 ],\n",
       "        [ 1.53563534],\n",
       "        [-3.31256276]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11999 entries, 9838 to 7270\n",
      "Data columns (total 18 columns):\n",
      "Work_accident_1            11999 non-null uint8\n",
      "average_montly_hours       11999 non-null int64\n",
      "department_RandD           11999 non-null uint8\n",
      "department_accounting      11999 non-null uint8\n",
      "department_hr              11999 non-null uint8\n",
      "department_management      11999 non-null uint8\n",
      "department_marketing       11999 non-null uint8\n",
      "department_product_mng     11999 non-null uint8\n",
      "department_sales           11999 non-null uint8\n",
      "department_support         11999 non-null uint8\n",
      "department_technical       11999 non-null uint8\n",
      "last_evaluation            11999 non-null float64\n",
      "number_project             11999 non-null int64\n",
      "promotion_last_5years_1    11999 non-null uint8\n",
      "salary_low                 11999 non-null uint8\n",
      "salary_medium              11999 non-null uint8\n",
      "satisfaction_level         11999 non-null float64\n",
      "time_spend_company         11999 non-null int64\n",
      "dtypes: float64(2), int64(3), uint8(13)\n",
      "memory usage: 714.8 KB\n"
     ]
    }
   ],
   "source": [
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Work_accident_1</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>department_RandD</th>\n",
       "      <th>department_accounting</th>\n",
       "      <th>department_hr</th>\n",
       "      <th>department_management</th>\n",
       "      <th>department_marketing</th>\n",
       "      <th>department_product_mng</th>\n",
       "      <th>department_sales</th>\n",
       "      <th>department_support</th>\n",
       "      <th>department_technical</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>promotion_last_5years_1</th>\n",
       "      <th>salary_low</th>\n",
       "      <th>salary_medium</th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>time_spend_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9838</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7689</th>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6557</th>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872</th>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Work_accident_1  average_montly_hours  department_RandD  \\\n",
       "9838                0                   188                 0   \n",
       "7689                0                   196                 0   \n",
       "6557                0                   175                 1   \n",
       "6872                0                   112                 0   \n",
       "820                 0                   284                 0   \n",
       "\n",
       "      department_accounting  department_hr  department_management  \\\n",
       "9838                      0              0                      0   \n",
       "7689                      0              0                      0   \n",
       "6557                      0              0                      0   \n",
       "6872                      1              0                      0   \n",
       "820                       0              0                      0   \n",
       "\n",
       "      department_marketing  department_product_mng  department_sales  \\\n",
       "9838                     0                       1                 0   \n",
       "7689                     0                       0                 0   \n",
       "6557                     0                       0                 0   \n",
       "6872                     0                       0                 0   \n",
       "820                      0                       0                 0   \n",
       "\n",
       "      department_support  department_technical  last_evaluation  \\\n",
       "9838                   0                     0             0.61   \n",
       "7689                   0                     1             0.78   \n",
       "6557                   0                     0             0.80   \n",
       "6872                   0                     0             0.86   \n",
       "820                    0                     1             0.93   \n",
       "\n",
       "      number_project  promotion_last_5years_1  salary_low  salary_medium  \\\n",
       "9838               3                        0           1              0   \n",
       "7689               4                        0           0              0   \n",
       "6557               3                        0           0              1   \n",
       "6872               4                        0           0              1   \n",
       "820                7                        0           1              0   \n",
       "\n",
       "      satisfaction_level  time_spend_company  \n",
       "9838                1.00                   4  \n",
       "7689                0.16                   5  \n",
       "6557                0.80                   2  \n",
       "6872                0.66                   6  \n",
       "820                 0.11                   4  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
