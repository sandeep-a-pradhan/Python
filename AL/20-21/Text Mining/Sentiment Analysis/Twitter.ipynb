{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In order to build a sentiment analyzer, first we need to equip ourselves with the right tools and methods. **Machine learning** is one such tool where people have developed various methods to classify. Classifiers may or may not need training data. \n",
    "\n",
    "In particular, we will deal with the following machine learning classifiers, namely, *Naive Bayes Classifier, Maximum Entropy Classifier* and *Support Vector Machines*. All of these classifiers require training data and hence these methods fall under the category of supervised classification.\n",
    "\n",
    "<img src=\"https://www.ravikiranj.net/images/supervised-classification.png\", width = 640,height = 480>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Classifiers\n",
    "The classifiers need to be trained and to do that, we need to list manually classified tweets. Let's start with 3 positive, 3 neutral and 3 negative tweets.\n",
    "\n",
    "### Positive tweets\n",
    "> *@PrincessSuperC Hey Cici sweetheart! Just wanted to let u know I luv u! OH! and will the mixtape drop soon? FANTASY RIDE MAY 5TH!!!!*\n",
    "\n",
    "> *@Msdebramaye I heard about that contest! Congrats girl!!*\n",
    "\n",
    "> *UNC!!! NCAA Champs!! Franklin St.: I WAS THERE!! WILD AND CRAZY!!!!!! Nothing like it...EVER* http://tinyurl.com/49955t3\n",
    "\n",
    "### Neutral tweets\n",
    "> *Do you Share More #jokes #quotes #music #photos or #news #articles on #Facebook or #Twitter?*\n",
    "\n",
    "> *Good night #Twitter and #TheLegionoftheFallen. 5:45am cimes awfully early!*\n",
    "\n",
    "> *I just finished a 2.66 mi run with a pace of 11'14\"/mi with Nike+ GPS. #nikeplus #makeitcount*\n",
    "\n",
    "### Negative tweets\n",
    "> Disappointing day. Attended a car boot sale to raise some funds for the sanctuary, made a total of 88p after the entry fee - sigh\n",
    "\n",
    "> no more taking Irish car bombs with strange Australian women who can drink like rockstars...my head hurts.\n",
    "\n",
    "> Just had some bloodwork done. My arm hurts\n",
    "\n",
    "## Preprocess tweets\n",
    "\n",
    "1. Lower Case - Convert the tweets to lower case.\n",
    "2. URLs - I don't intend to follow the short urls and determine the content of the site, so we can eliminate all of these URLs via regular expression matching or replace with generic word URL.\n",
    "3. @username - we can eliminate \"@username\" via regex matching or replace it with generic word AT_USER.\n",
    "4. #hashtag - hash tags can give us some useful information, so it is useful to replace them with the exact same word without the hash. E.g. #nike replaced with 'nike'.\n",
    "5. Punctuations and additional white spaces - remove punctuation at the start and ending of the tweets. E.g: ' the day is beautiful! ' replaced with 'the day is beautiful'. It is also helpful to replace multiple whitespaces with a single whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vector\n",
    "\n",
    "In pattern recognition and machine learning, a _**feature vector** is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis._\n",
    "\n",
    "Feature vector is the most important concept in implementing a classifier. A good feature vector directly determines how successful your classifier will be. The feature vector is used to build a model which the classifier learns from the training data and further can be used to classify previously unseen data.\n",
    "\n",
    "In tweets, we can use the presence/absence of words that appear in tweet as features. In the training data, consisting of positive, negative and neutral tweets, we can split each tweet into words and add each word to the feature vector. Some of the words might not have any contribution in indicating the sentiment of a tweet and hence we can filter them out. _Adding individual (single) words to the feature vector is referred to as **'unigrams'** approach._\n",
    "\n",
    "Some of the other feature vectors also add 'bi-grams' in combination with 'unigrams'. \n",
    "\n",
    "For example, 'not good' (bigram) completely changes the sentiment compared to adding 'not' and 'good' individually. Here, for simplicity, we will only consider the unigrams. \n",
    "\n",
    "Before adding the words to the feature vector, we need to preprocess them in order to filter, otherwise, the feature vector will explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering tweet words (for feature vector)\n",
    "1. Stop words - a, is, the, with etc. The full list of stop words is present in a text file attached with the project. These words don't indicate any sentiment and can be removed.\n",
    "2. Repeating letters - if you look at the tweets, sometimes people repeat letters to stress the emotion. E.g. hunggrryyy, huuuuuuungry for 'hungry'. We can look for 2 or more repetitive letters in words and replace them by 2 of the same.\n",
    "3. Punctuation - we can remove punctuation such as comma, single/double quote, question marks at the start and end of each word. E.g. beautiful!!!!!! replaced with beautiful\n",
    "4. Words must start with an alphabet - For simplicity sake, we can remove all those words which don't start with an alphabet. E.g. 15th, 5.34am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling tweets from twitter\n",
    "\n",
    "Crawl Tweets Against Hash Tags\n",
    "\n",
    "To have access to the Twitter API, you'll need to login the Twitter Developer website and create an application. Enter your desired Application Name, Description and your website address making sure to enter the full address including the http://. You can leave the callback URL empty.\n",
    "\n",
    "<img src = http://ipullrank.com/wp-content/uploads/2017/04/create-an-application.png>\n",
    "\n",
    "---\n",
    "\n",
    "After registering, create an access token and grab your application’s Consumer Key, Consumer Secret, Access token and Access token secret from Keys and Access Tokens tab.\n",
    "\n",
    "<img src = http://ipullrank.com/wp-content/uploads/2017/04/application-settings2.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import twython\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "####input your credentials here\n",
    "consumer_key = 'orMX860DHi6kNaWRUdLPpUZLd'\n",
    "consumer_secret = 'sUODNhBoUHbLlwIlnty32RguQ2ElcpcFGymnyzxkvI7Dogsi4v'\n",
    "access_token = '58457005-BFCQKaAWsaYl7JFNYVsNSmYNhNw0fpUlvcRrySsYN'\n",
    "access_token_secret = 'r6QFQKuYOFxQPYwmM5lWcTjNMzynx6CAsHGUe6Hmp1xMd'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "#####Kotak Bank\n",
    "# Open/Create a file to append data\n",
    "csvFile = open('C:/Users/admin/Desktop/Training Material/8. Data Science With Python/20. Text Mining - NLP/Sentiment Analysis/kotak.csv', 'w')\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief intro to function that extracts tweets based on hashtag, userID or page : `tweepy.Cursor()` function\n",
    "\n",
    "** User Time Line (the @someone thing) **\n",
    "\n",
    "`tweepy.Cursor(api.user_timeline, id=\"twitter\")`\n",
    "\n",
    "\n",
    "** Generic Serach for @username or a #hashtag **\n",
    "\n",
    "`API.search(q[, lang ][, locale ][, rpp ][, page ][, since_id ][, geocode ][, show_user])`\n",
    "Returns tweets that match a specified query.\n",
    "\n",
    "Parameters\n",
    "\n",
    "• q – the search query string\n",
    "\n",
    "• lang – Restricts tweets to the given language, given by an ISO 639-1 code.\n",
    "\n",
    "• locale – Specify the language of the query you are sending. This is intended for languagespecific\n",
    "clients and the default should work in the majority of cases.\n",
    "\n",
    "• rpp – The number of tweets to return per page, up to a max of 100.\n",
    "\n",
    "• page – The page number (starting at 1) to return, up to a max of roughly 1500 results (based\n",
    "on rpp * page.\n",
    "\n",
    "• since_id – Returns only statuses with an ID greater than (that is, more recent than) the\n",
    "specified ID.\n",
    "\n",
    "• geocode – Returns tweets by users located within a given radius of the given latitude/longitude.\n",
    "The location is preferentially taking from the Geotagging API, but will fall\n",
    "back to their Twitter profile. The parameter value is specified by “latitide,longitude,radius”,\n",
    "where radius units must be specified as either “mi” (miles) or “km” (kilometers). Note that\n",
    "you cannot use the near operator via the API to geocode arbitrary locations; however you\n",
    "can use this geocode parameter to search near geocodes directly.\n",
    "\n",
    "• show_user – When true, prepends “<user>:” to the beginning of the tweet. This is useful\n",
    "for readers that do not display Atom’s author field. The default is false.\n",
    "\n",
    "For more info, please refer to the official documentation of tweepy in the link :\n",
    "https://media.readthedocs.org/pdf/tweepy/latest/tweepy.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Search tweets by a hash tag.\n",
    "\n",
    "date_c = list()\n",
    "tweet_s = list()\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#kotak\",count=100,\n",
    "                           lang=\"en\",\n",
    "                           since=\"2017-05-01\").items():\n",
    "    date_c.append(tweet.created_at)\n",
    "    tweet_s.append(tweet.text)\n",
    "    # print (tweet.created_at, tweet.text)\n",
    "    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Preprocess tweets\n",
    "def processTweet2(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet    \n",
    "\n",
    "###get stopword list\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "stopWords = []\n",
    "\n",
    "st = open('C:/Users/admin/Desktop/Training Material/8. Data Science With Python/20. Text Mining - NLP/Sentiment Analysis/stopwords.txt', 'r')\n",
    "stopWords = getStopWordList('C:/Users/admin/Desktop/Training Material/8. Data Science With Python/20. Text Mining - NLP/Sentiment Analysis/stopwords.txt')\n",
    "\n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "def getFeatureVector(tweet):\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector\n",
    " \n",
    "###load airline sentiment training data \n",
    "    \n",
    "airlinetrain = pd.read_csv(\"Airline-Sentiment-2-w-AA.csv\", encoding =\"ISO-8859-1\")\n",
    "tweets = []\n",
    "featureList = []\n",
    "for i in range(len(airlinetrain)):\n",
    "    sentiment = airlinetrain['airline_sentiment'][i]\n",
    "    tweet = airlinetrain['text'][i]\n",
    "    processedTweet = processTweet2(tweet)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    featureList.extend(featureVector)\n",
    "    tweets.append((featureVector, sentiment))\n",
    "        \n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "#end\n",
    "\n",
    "### Remove featureList duplicates\n",
    "featureList = list(set(featureList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = pd.read_csv(\"C:/Users/admin/Desktop/Training Material/8. Data Science With Python/20. Text Mining - NLP/Sentiment Analysis/kotak.csv\")\n",
    "ua.columns = [\"Date\",\"tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweets)\n",
    "# Train the classifier Naive Bayes Classifier\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "#ua is a dataframe containing all the tweets\n",
    "\n",
    "ua['sentiment'] = ua['tweets'].apply(lambda tweet: NBClassifier.classify(extract_features(getFeatureVector(processTweet2(tweet)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
